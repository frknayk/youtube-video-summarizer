experiment_name: TuneHyperparams

# This configuration will try each model architecture with different batch sizes and optimizers.
# The best model will be selected based on the validation accuracy.

# Model configuration
model:
  # Model architecture name and class name must macthed.
  architectures: [
    architecture_1, 
    architecture_2,
    architecture_3,
    architecture_4]


training:
  batch_size: [16,32,64,128]
  epochs: 30 # First 30 epoch is enough

# Try out different configurations
optimizer_configs:
  config_1:
    type: Adam
    learning_rate: 0.0001
    betas: [0.9, 0.999]
  config_2:
    type: SGD
    momentum: 0.9
    weight_decay: 1e-4

# Evaluation configuration
evaluation:
  batch_size: 32


# Data configuration, consumed by dataset_loader.py
data:
  name: Dataset_NAME
  path_train_dataset: data/Dataset_NAME/Train # Dataset path for training
  path_inference_dataset: data/Dataset_NAME/Test # Dataset path for inference(no labels)
  num_classes: 4
  split_ratio:
    train_ratio: 0.8 # 80% for training
    val_ratio: 0.1 # 10% for testing and 10% for validation
  transform:
    normalization_params:
      train:
        mean: [0.5, 0.5, 0.5]
        std: [0.5, 0.5, 0.5]
      test:
        mean: [0.5, 0.5, 0.5]
        std: [0.5, 0.5, 0.5]
      validation:
        mean: [0.5, 0.5, 0.5]
        std: [0.5, 0.5, 0.5]
    RandomHorizontalFlip: 0.4 # probability
    RandomVerticalFlip: 0.4 # probability
    RandomRotation: [0, 180] # degrees
    resize: 448
  random_seed: 42


