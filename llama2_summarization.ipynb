{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcript\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import os\n",
    "# Summarization\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    AutoProcessor,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import torch\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transcript Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_transcript(video_url):\n",
    "    try:\n",
    "        # Extract the video ID from the URL\n",
    "        video_id = video_url.split(\"v=\")[1]\n",
    "        # Fetch the transcript for the video\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        # Process the transcript data\n",
    "        text_transcript = \"\\n\".join([entry['text'] for entry in transcript])\n",
    "        return text_transcript\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "import re \n",
    "def clean_transcript(transcript):\n",
    "    # Remove non-speech elements (e.g., laughter, background noises)\n",
    "    transcript = re.sub(r'\\[.*?\\]', '', transcript)\n",
    "\n",
    "    # Correct spelling and grammar (you can use libraries like NLTK or spaCy for this)\n",
    "    # Example:\n",
    "    # import nltk\n",
    "    # transcript = ' '.join(nltk.word_tokenize(transcript))\n",
    "\n",
    "    # Normalize punctuation and formatting\n",
    "    transcript = transcript.replace('\\n', ' ')  # Remove line breaks\n",
    "    transcript = re.sub(r'\\s+', ' ', transcript)  # Remove extra whitespaces\n",
    "\n",
    "    # Remove timestamps and annotations\n",
    "    transcript = re.sub(r'\\[\\d+:\\d+:\\d+\\]', '', transcript)\n",
    "\n",
    "    # Handle speaker identification (if present)\n",
    "    # Example: transcript = re.sub(r'Speaker\\d+:', '', transcript)\n",
    "\n",
    "    # Remove filler words and phrases\n",
    "    filler_words = ['like', 'you know', 'sort of']  # Add more as needed\n",
    "    for word in filler_words:\n",
    "        transcript = transcript.replace(word, '')\n",
    "    \n",
    "    # Replace common contractions with their expanded forms\n",
    "    transcript = transcript.replace(\"won't\", \"will not\")\n",
    "    transcript = transcript.replace(\"can't\", \"cannot\")\n",
    "    transcript = transcript.replace(\"n't\", \" not\")\n",
    "    transcript = transcript.replace(\"'ll\", \" will\")\n",
    "    transcript = transcript.replace(\"'ve\", \" have\")\n",
    "    transcript = transcript.replace(\"'re\", \" are\")\n",
    "    transcript = transcript.replace(\"'d\", \" would\")\n",
    "    transcript = transcript.replace(\"'s\", \" is\")\n",
    "\n",
    "    return transcript.strip()  # Trim leading/trailing whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Summarization LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(system_prompt, text):\n",
    "    \"\"\"\n",
    "    It is not a good practice to load the model again and again,\n",
    "    but for the sake of simlicity for demo, let's keep as it is\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the model name to be used for the chat function\n",
    "    model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    # Authentication token for Hugging Face API\n",
    "    # token = os.environ['HUGGINGFACE_TOKEN']\n",
    "\n",
    "    # # Configure the model to load in a quantized 8-bit format for efficiency\n",
    "    # bnb_config = BitsAndBytesConfig(\n",
    "    #     load_in_8bit=True\n",
    "    # )\n",
    "\n",
    "    # # Set the device map to load the model on GPU 0\n",
    "    # device_map = {\"\": 0}\n",
    "    # # Load the model from Hugging Face with the specified configuration\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_name,\n",
    "    #     quantization_config=bnb_config,\n",
    "    #     device_map=device_map)\n",
    "\n",
    "    # # Load the tokenizer for the model\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "\n",
    "    # # Create a text-generation pipeline with the loaded model and tokenizer\n",
    "    # llama_pipeline = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    llama_pipeline = pipeline(\n",
    "        \"text-generation\", #task\n",
    "        model=model_name,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        # max_length=max_token_length,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Format the input text with special tokens for the model\n",
    "    text = f\"\"\"\n",
    "    <s>[INST] <<SYS>>\n",
    "    {system_prompt}\n",
    "    <</SYS>>\n",
    "    {text}[/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate sequences using the pipeline with specified parameters\n",
    "    sequences = llama_pipeline(\n",
    "        text,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=32000\n",
    "    )\n",
    "\n",
    "    # Extract the generated text from the sequences\n",
    "    generated_text = sequences[0][\"generated_text\"]\n",
    "    # Trim the generated text to remove the instruction part\n",
    "    generated_text = generated_text[generated_text.find('[/INST]')+len('[/INST]'):]\n",
    "\n",
    "    # Return the processed generated text\n",
    "    return generated_text\n",
    "\n",
    "def summarize(text):\n",
    "    # Define the maximum input length for each iteration of summarization\n",
    "    input_len = 10000\n",
    "\n",
    "    # Start an infinite loop to repeatedly summarize the text\n",
    "    while True:\n",
    "        # Print the current length of the text\n",
    "        print(len(text))\n",
    "        # Call the chat function to summarize the text. Only the first 'input_len' characters are considered for summarization\n",
    "        summary = chat(\"\", \"Summarize the following: \" + text[0:input_len])\n",
    "\n",
    "        if len(text) < input_len:\n",
    "            return summary\n",
    "        \n",
    "        # Concatenate the current summary with the remaining part of the text for the next iteration\n",
    "        text = summary + \" \" + text[input_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_text_cleaned = clean_transcript(fetch_transcript(\"https://www.youtube.com/watch?v=48jlHaxZnig\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1bb5299337f4c24a449362718f6ddbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     The article discusses the findings from the addiction literature that can help overcome procrastination. The key is to put oneself in a more painful state than the current state of procrastination. This can be achieved by engaging in activities that are harder or more uncomfortable than the current state, such as taking a cold shower or immersion. The idea is that by increasing the friction or discomfort, it can help rebound the dopamine trough more quickly, thus motivating the individual to take action. The article emphasizes the importance of finding a tool that is safe and uncomfortable, but not damaging, to help steep the trough and bring the individual back to their baseline level of dopamine more quickly.\n"
     ]
    }
   ],
   "source": [
    "podcast_summary = summarize(transcript_text_cleaned)\n",
    "print(podcast_summary)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
