{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcript\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import os\n",
    "# Summarization\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    AutoProcessor,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transcript Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_transcript(video_url):\n",
    "    try:\n",
    "        # Extract the video ID from the URL\n",
    "        video_id = video_url.split(\"v=\")[1]\n",
    "        # Fetch the transcript for the video\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        # Process the transcript data\n",
    "        text_transcript = \"\\n\".join([entry['text'] for entry in transcript])\n",
    "        return text_transcript\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "import re \n",
    "def clean_transcript(transcript):\n",
    "    # Remove non-speech elements (e.g., laughter, background noises)\n",
    "    transcript = re.sub(r'\\[.*?\\]', '', transcript)\n",
    "\n",
    "    # Correct spelling and grammar (you can use libraries like NLTK or spaCy for this)\n",
    "    # Example:\n",
    "    # import nltk\n",
    "    # transcript = ' '.join(nltk.word_tokenize(transcript))\n",
    "\n",
    "    # Normalize punctuation and formatting\n",
    "    transcript = transcript.replace('\\n', ' ')  # Remove line breaks\n",
    "    transcript = re.sub(r'\\s+', ' ', transcript)  # Remove extra whitespaces\n",
    "\n",
    "    # Remove timestamps and annotations\n",
    "    transcript = re.sub(r'\\[\\d+:\\d+:\\d+\\]', '', transcript)\n",
    "\n",
    "    # Handle speaker identification (if present)\n",
    "    # Example: transcript = re.sub(r'Speaker\\d+:', '', transcript)\n",
    "\n",
    "    # Remove filler words and phrases\n",
    "    filler_words = ['like', 'you know', 'sort of']  # Add more as needed\n",
    "    for word in filler_words:\n",
    "        transcript = transcript.replace(word, '')\n",
    "    \n",
    "    # Replace common contractions with their expanded forms\n",
    "    transcript = transcript.replace(\"won't\", \"will not\")\n",
    "    transcript = transcript.replace(\"can't\", \"cannot\")\n",
    "    transcript = transcript.replace(\"n't\", \" not\")\n",
    "    transcript = transcript.replace(\"'ll\", \" will\")\n",
    "    transcript = transcript.replace(\"'ve\", \" have\")\n",
    "    transcript = transcript.replace(\"'re\", \" are\")\n",
    "    transcript = transcript.replace(\"'d\", \" would\")\n",
    "    transcript = transcript.replace(\"'s\", \" is\")\n",
    "\n",
    "    return transcript.strip()  # Trim leading/trailing whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Summarization LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_transcript(text, llama_pipeline):\n",
    "    def summarize_text(llama_pipeline, system_prompt, text):\n",
    "        # Format the input text with special tokens for the model\n",
    "        text = f\"\"\"\n",
    "        <s>[INST] <<SYS>>\n",
    "        {system_prompt}\n",
    "        <</SYS>>\n",
    "        {text}[/INST]\n",
    "        \"\"\"\n",
    "        # Generate sequences using the pipeline with specified parameters\n",
    "        sequences = llama_pipeline(text)\n",
    "        # Extract the generated text from the sequences\n",
    "        generated_text = sequences[0][\"generated_text\"]\n",
    "        # Trim the generated text to remove the instruction part\n",
    "        generated_text = generated_text[generated_text.find('[/INST]')+len('[/INST]'):]\n",
    "        # Return the processed generated text\n",
    "        return generated_text\n",
    "    # Define the maximum input length for each iteration of summarization\n",
    "    input_len = 1000\n",
    "    # Start an infinite loop to repeatedly summarize the text\n",
    "    while True:\n",
    "        # Print the current length of the text\n",
    "        print(len(text))\n",
    "        # Call the chat function to summarize the text. Only the first 'input_len' characters are considered for summarization\n",
    "        summary = summarize_text(llama_pipeline, \"\", \"Summarize the following: \" + text[0:input_len])\n",
    "        if len(summary) < input_len:\n",
    "            return summary\n",
    "        # Concatenate the current summary with the remaining part of the text for the next iteration\n",
    "        text = summary + \" \" + text[input_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae5f7e23a024310ac6722a780ca4c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # Define the model name to be used for the chat function\n",
    "    model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    pipeline_llama2 = pipeline(\n",
    "        \"text-generation\", #task\n",
    "        model=model_name,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        # max_length=max_token_length,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     Summary:\n",
      "\n",
      "The initial state of a reseted Simics system has several important settings:\n",
      "\n",
      "1. CR0: Mostly all zeros, with the exception of the protection enable bit, which is set to zero, indicating that the system is in real mode.\n",
      "2. CR3: Zero.\n",
      "3. CR4: Zero.\n",
      "4. CR2: Zero.\n",
      "5. CR1: Zero.\n",
      "\n",
      "The bits seen in the architecture 2001 and the PE are also present in the initial state.\n",
      "\n",
      "The cache disable and non-write through bits are set to ones, indicating that the cache should not be used and that reads should be performed directly from the flash.\n",
      "\n",
      "It is important to note that the initial state of a virtual machine may not be the same as a physical machine, and that certain attacks have been demonstrated in the past where an attacker could write into the virtual BIOS and persist across reset. Therefore, it is important to always validate the cache and ensure that the system is in a known good state after reset.\n"
     ]
    }
   ],
   "source": [
    "transcript = clean_transcript(fetch_transcript(\"https://www.youtube.com/watch?v=Vst889H1V2I\"))\n",
    "podcast_summary = summarize_transcript(transcript, pipeline_llama2)\n",
    "print(podcast_summary)\n",
    "with open('transcript.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write(podcast_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
